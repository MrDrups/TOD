{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymorphy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13928\\3988745296.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy2'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с величайшим усилием выбравшись из потока убегающих людей кутузов со свитой уменьшившейся вдвое поехал на звуки выстрелов русских орудий\n",
      "с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий\n"
     ]
    }
   ],
   "source": [
    "with open (\"./data/litw-win.txt\", \"r\", encoding='windows-1251') as file:\n",
    "    words = [i.split()[-1] for i in file]\n",
    "    \n",
    "    \n",
    "def closest_word_by_Levenshtein(word, words):\n",
    "    closest = None\n",
    "    closest_distance = float('inf')\n",
    "    for w in words:\n",
    "        distance = Levenshtein.distance(word, w)\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest = w\n",
    "    return closest\n",
    "\n",
    "\n",
    "def correct_text(text, words):\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        if word not in words:\n",
    "            closest = closest_word_by_Levenshtein(word, words)\n",
    "            corrected.append(closest)\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''\n",
    "corrected = correct_text(text, words)\n",
    "print(corrected)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from snowballstemmer import stemmer\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [i for i in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с',\n",
       " 'велечайш',\n",
       " 'усил',\n",
       " 'выбра',\n",
       " 'из',\n",
       " 'поток',\n",
       " 'убега',\n",
       " 'люд',\n",
       " 'Кутуз',\n",
       " 'со',\n",
       " 'свит',\n",
       " 'уменьшевш',\n",
       " 'вдво',\n",
       " 'поеха',\n",
       " 'на',\n",
       " 'звук',\n",
       " 'выстрел',\n",
       " 'русск',\n",
       " 'оруд']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = stemmer('russian')\n",
    "stemmed_words = [stemmer.stemWord(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с',\n",
       " 'велечайший',\n",
       " 'усилие',\n",
       " 'выбираться',\n",
       " 'из',\n",
       " 'поток',\n",
       " 'убегать',\n",
       " 'человек',\n",
       " 'кутузов',\n",
       " 'со',\n",
       " 'свита',\n",
       " 'уменьшевшийся',\n",
       " 'вдвое',\n",
       " 'поехать',\n",
       " 'на',\n",
       " 'звук',\n",
       " 'выстрел',\n",
       " 'русский',\n",
       " 'орудие']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [m.lemmatize(word)[0] for word in words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 2 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1]]\n",
      "['litw', 'txt', 'win', 'words', 'ближайшие', 'все', 'данное', 'если', 'есть', 'заданном', 'заменив', 'запишите', 'из', 'исправьте', 'их', 'левенштейна', 'на', 'не', 'ним', 'опечатка', 'опечатками', 'опечатки', 'предложении', 'расстояния', 'слова', 'слове', 'слово', 'смысле', 'содержится', 'списка', 'списке', 'список', 'считайте', 'файла', 'что']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"Считайте слова из файла litw-win.txt и запишите их в список words.\",\n",
    "        \"В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна)к ним слова из списка words.\",\n",
    "        \"Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.\"\n",
    "        ]\n",
    "\n",
    "CV = CountVectorizer()\n",
    "\n",
    "vectors = CV.fit_transform(text)\n",
    "\n",
    "print(vectors.toarray())\n",
    "\n",
    "print(CV.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32868 30000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv('./data/preprocessed_descriptions.csv')\n",
    "\n",
    "df['preprocessed_descriptions']=df['preprocessed_descriptions'].apply(str)\n",
    "texts = list(df['preprocessed_descriptions'])\n",
    "\n",
    "words = set()\n",
    "for text in texts:\n",
    "    words.update(word_tokenize(text))\n",
    "\n",
    "words_all = list()\n",
    "for text in texts:\n",
    "    words_all.append(word_tokenize(text))\n",
    "print(len(words),len(words_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('./data/preprocessed_descriptions.csv')\n",
    "df['preprocessed_descriptions']=df['preprocessed_descriptions'].apply(str)\n",
    "# Список всех слов\n",
    "all_words = []\n",
    "for i in df['preprocessed_descriptions']:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    all_words += words\n",
    "\n",
    "# Набор уникальных слов\n",
    "words = set(all_words)\n",
    "\n",
    "print(f'Количество уникальных слов: {len(words)}')\n",
    "print(f'Общее количество слов: {len(all_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние между connoisseurs и selfraising: 11\n",
      "Расстояние между frittatasi и havnt: 9\n",
      "Расстояние между nava и budshese: 8\n",
      "Расстояние между associate и stacks: 7\n",
      "Расстояние между saucemakes и alfassia: 9\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import editdistance\n",
    "\n",
    "pairs = random.sample(list(words), 10)\n",
    "\n",
    "for i in range(0,len(pairs),2):\n",
    "    pair1,pair2 = pairs[i],pairs[i+1]\n",
    "    distance = editdistance.eval(pair1, pair2)\n",
    "    print(\"Расстояние между {} и {}: {}\".format(pair1, pair2, distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool', 'coop', 'cook', 'col', 'chol']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_k_nearest_words(word, words, k):\n",
    "    distances = [(w, editdistance.eval(word, w)) for w in words]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return [w[0] for w in distances[:k]]\n",
    "\n",
    "\n",
    "get_k_nearest_words(\"cool\", words, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             stemmed_word normalized_word\n",
      "word                                     \n",
      "johnny             johnni          johnny\n",
      "falls                fall            fall\n",
      "fear                 fear            fear\n",
      "ilanna             ilanna          ilanna\n",
      "heartwarming    heartwarm    heartwarming\n",
      "...                   ...             ...\n",
      "thailand         thailand        thailand\n",
      "sopressata     sopressata      sopressata\n",
      "batters            batter          batter\n",
      "crannies           cranni        crannies\n",
      "deteriorated     deterior     deteriorate\n",
      "\n",
      "[32868 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def lem_word(word):\n",
    "    return lemmatizer.lemmatize(word,\"v\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(words, columns=['word'])\n",
    "df['stemmed_word'] = df['word'].apply(stem_word)\n",
    "df['normalized_word'] = df['word'].apply(lem_word)\n",
    "\n",
    "df.set_index('word', inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До 1069885. После 581520. Доля 54.35.\n",
      "\n",
      "топ-10 слов до удаления стоп-слов\n",
      "\n",
      "1. the\n",
      "2. a\n",
      "3. and\n",
      "4. this\n",
      "5. i\n",
      "6. to\n",
      "7. is\n",
      "8. it\n",
      "9. of\n",
      "10. for\n",
      "\n",
      "топ-10 слов после удаления стоп-слов\n",
      "\n",
      "1. recipe\n",
      "2. make\n",
      "3. time\n",
      "4. use\n",
      "5. great\n",
      "6. like\n",
      "7. easy\n",
      "8. one\n",
      "9. made\n",
      "10. good\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# вычисление доли стоп-слов\n",
    "all_words_filtered = [word for word in all_words if word not in stop_words]\n",
    "print(\"До {}. После {}. Доля {}.\".format(len(all_words), len(all_words_filtered),round(len(all_words_filtered)/len(all_words)*100,2)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nтоп-10 слов до удаления стоп-слов\\n\")\n",
    "counter_all = Counter(all_words)\n",
    "top_10 = counter_all.most_common(10)\n",
    "k = 1\n",
    "for word, count in top_10:\n",
    "    print(\"{}. {}\".format(k,word))\n",
    "    k+=1\n",
    "print(\"\\nтоп-10 слов после удаления стоп-слов\\n\")\n",
    "counter_all_fil = Counter(all_words_filtered)\n",
    "top_10_fil = counter_all_fil.most_common(10)\n",
    "k = 1\n",
    "for word, count in top_10_fil:\n",
    "    print(\"{}. {}\".format(k,word))\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рецепт 1517:\n",
      "given by a friend and i find the difference in this dip enjoyable\n",
      "Вектор:\n",
      "[[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
      "  0.30151134 0.30151134 0.30151134 0.30151134 0.30151134]]\n",
      "\n",
      "Рецепт 1075:\n",
      "light summer dessert\n",
      "Вектор:\n",
      "[[0.57735027 0.57735027 0.57735027]]\n",
      "\n",
      "Рецепт 16243:\n",
      "i have been making this for years  it is very tasty chicken recipe and one that isnt to be missed try it and see for yourself all the great mix of flavors in this tender juicy chicken\n",
      "Вектор:\n",
      "[[0.14744196 0.29488391 0.14744196 0.14744196 0.29488391 0.14744196\n",
      "  0.29488391 0.14744196 0.14744196 0.14744196 0.14744196 0.14744196\n",
      "  0.29488391 0.14744196 0.14744196 0.14744196 0.14744196 0.14744196\n",
      "  0.14744196 0.14744196 0.14744196 0.14744196 0.14744196 0.14744196\n",
      "  0.14744196 0.29488391 0.14744196 0.14744196 0.14744196 0.14744196\n",
      "  0.14744196]]\n",
      "\n",
      "Рецепт 4323:\n",
      "this is a tasty filling and inexpensive dishfrom the lone star cookbook 199798 aggie moms club texarkana tx\n",
      "Вектор:\n",
      "[[0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563 0.24253563 0.24253563 0.24253563]]\n",
      "\n",
      "Рецепт 10427:\n",
      "this recipe is a specialty of italys southern regions preparation time does not include the hour needed for salted eggplant slices to drain\n",
      "Вектор:\n",
      "[[0.21320072 0.21320072 0.21320072 0.21320072 0.21320072 0.21320072\n",
      "  0.21320072 0.21320072 0.21320072 0.21320072 0.21320072 0.21320072\n",
      "  0.21320072 0.21320072 0.21320072 0.21320072 0.21320072 0.21320072\n",
      "  0.21320072 0.21320072 0.21320072 0.21320072]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# загрузка данных из файла\n",
    "data = pd.read_csv('./data/preprocessed_descriptions.csv')\n",
    "data['preprocessed_descriptions']=data['preprocessed_descriptions'].apply(str)\n",
    "# выбор 5 случайных рецептов\n",
    "random_recipes = data.sample(5)\n",
    "\n",
    "# создание объекта TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# преобразование описания каждого рецепта в числовой вектор\n",
    "for i, row in random_recipes.iterrows():\n",
    "    description = row['preprocessed_descriptions']\n",
    "    vector = vectorizer.fit_transform([description])\n",
    "    print(f\"Рецепт {i+1}:\\n{description}\\nВектор:\\n{vector.toarray()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Recipes</th>\n",
       "      <th>1516</th>\n",
       "      <th>1074</th>\n",
       "      <th>16242</th>\n",
       "      <th>4322</th>\n",
       "      <th>10426</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recipes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956913</td>\n",
       "      <td>0.984280</td>\n",
       "      <td>0.988199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16242</th>\n",
       "      <td>0.956913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.953010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>0.984280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10426</th>\n",
       "      <td>0.988199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Recipes     1516   1074      16242     4322      10426\n",
       "Recipes                                               \n",
       "1516     0.000000    1.0  0.956913  0.984280  0.988199\n",
       "1074     1.000000    0.0  1.000000  1.000000  1.000000\n",
       "16242    0.956913    1.0  0.000000  0.954687  0.953010\n",
       "4322     0.984280    1.0  0.954687  0.000000  0.989145\n",
       "10426    0.988199    1.0  0.953010  0.989145  0.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(data['preprocessed_descriptions'])\n",
    "\n",
    "# получаем матрицу векторов\n",
    "matrix = X.toarray()\n",
    "\n",
    "# вычисляем близость между каждой парой рецептов\n",
    "distances = cosine_distances(matrix[random_recipes.index])\n",
    "\n",
    "# формируем DataFrame с результатами\n",
    "result_df = pd.DataFrame(distances, index=random_recipes.index, columns=random_recipes.index)\n",
    "result_df.columns.name = 'Recipes'\n",
    "result_df.index.name = 'Recipes'\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>george s at the cove  black bean soup</td>\n",
       "      <td>an original recipe created by chef scott meska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>healthy for them  yogurt popsicles</td>\n",
       "      <td>my children and their friends ask for my homem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i can t believe it s spinach</td>\n",
       "      <td>these were so go it surprised even me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>italian  gut busters</td>\n",
       "      <td>my sisterinlaw made these for us at a family g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>zwetschgenkuchen  bavarian plum cake</td>\n",
       "      <td>this is a traditional fresh plum cake thought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>zwiebelkuchen   southwest german onion cake</td>\n",
       "      <td>this is a traditional late summer early fall s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>this is a delicious soup that i originally fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>ive heard of the cookies by design company but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          name  \\\n",
       "0               0         george s at the cove  black bean soup   \n",
       "1               1            healthy for them  yogurt popsicles   \n",
       "2               2                  i can t believe it s spinach   \n",
       "3               3                          italian  gut busters   \n",
       "4               4      love is in the air  beef fondue   sauces   \n",
       "...           ...                                           ...   \n",
       "29995       29995  zurie s holey rustic olive and cheddar bread   \n",
       "29996       29996          zwetschgenkuchen  bavarian plum cake   \n",
       "29997       29997   zwiebelkuchen   southwest german onion cake   \n",
       "29998       29998                                   zydeco soup   \n",
       "29999       29999        cookies by design   cookies on a stick   \n",
       "\n",
       "                               preprocessed_descriptions  \n",
       "0      an original recipe created by chef scott meska...  \n",
       "1      my children and their friends ask for my homem...  \n",
       "2                  these were so go it surprised even me  \n",
       "3      my sisterinlaw made these for us at a family g...  \n",
       "4      i think a fondue is a very romantic casual din...  \n",
       "...                                                  ...  \n",
       "29995  this is based on a french recipe but i changed...  \n",
       "29996  this is a traditional fresh plum cake thought ...  \n",
       "29997  this is a traditional late summer early fall s...  \n",
       "29998  this is a delicious soup that i originally fou...  \n",
       "29999  ive heard of the cookies by design company but...  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Рецепт 1074 и 16242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
